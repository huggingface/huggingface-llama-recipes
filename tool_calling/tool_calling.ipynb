{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ke2KYme3Iuq"
      },
      "source": [
        "# Tool Calling using Llama 3.2 üîß\n",
        "\n",
        "In this notebook, we'll explore the **tool-calling capabilities** of the **Llama 3.2 models**. Our goal is to show how the tool system works under the hood, so you have a clear understanding of its capabilities. For deployment options, you may want to consider [llama-stack-apps](https://github.com/meta-llama/llama-stack-apps) and [llama-stack](https://github.com/meta-llama/llama-stack). For more details about the model, you may also consider reviewing the official documentation for [tool calling with Llama 3.2](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-tool-calling-(1b/3b)-).\n",
        "\n",
        "Let's get started! üöÄ  \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djxagjcldf32"
      },
      "source": [
        "## 1. Setup and Imports üõ†Ô∏è  \n",
        "\n",
        "Before we dive in, let's start by installing the necessary dependencies to ensure our environment is ready for the tool-calling examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8iDuJRH5u8b"
      },
      "outputs": [],
      "source": [
        "!pip install -q llama-stack"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8NbB1RY4H5e"
      },
      "source": [
        "We'll login to **Hugging Face** since these models are gated. If you haven't requested access yet, make sure to do so and confirm that it has been granted.  \n",
        "\n",
        "For more details, refer to the [Model Card](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "76d34dad28ce45e48806a4b829898819",
            "cb4e63441d2e45be9d5e6804e82830b9",
            "e107d71395394b6cac001a5e323ef6b8",
            "228ffff7dcaf4ac593364194c53d3547",
            "ad32ecb235c5422e846147b8c6481903",
            "cc4303145b234066a517330b66fed071",
            "33cef11df22f444e98043a408943c91a",
            "a57d8cef1dfe438f8a244bd0e546a04f",
            "e6ef504d91884912bb6f19ded832783c",
            "695d84785a7f43feabcb5571cec4c260",
            "14524c078b48471896597f97a071fc35",
            "1497c470009b486dba1dca51a9d2ce60",
            "3cd7c5bc91854604b3c96af7e4c1f464",
            "601ca2c9b03946cc83600c518ab69b55",
            "60daa7077c2344eeb0538860d0d64e26",
            "e6baa1b9954a4cd9bbd04179ab675a9b",
            "d98183520d9441ed860171ac55ad786f",
            "784fecb071f84b628bb6f38c3311b880",
            "151fd97a9c7149eb8c97c5ba48b7a6d6",
            "7e201d782ac24d578a9476e37c01240c"
          ]
        },
        "id": "hN1lhibi30Er",
        "outputId": "004b28b3-26d5-426a-e451-1116ce49d011"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "a8BZfMLYdhbF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Op3lUnaLmE-R"
      },
      "source": [
        "For our experiments, we'll use the base model **`meta-llama/Llama-3.2-3B-Instruct`** and demonstrate how to complete tool invocations.  \n",
        "\n",
        "The **Llama 3.2 family** includes a variety of models, each tailored for specific use cases. You can explore the entire collection in this [Llama 3.2 family collection](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf).  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YQcl647Ddu8M"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VE-Ejy4vrQpI"
      },
      "source": [
        "We'll define a `generate` function to streamline our workflow. This function will handle model calls using a given prompt and return the model's response. We'll use it throughout the notebook for consistency and ease of use.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "aIeGRPdLhY54"
      },
      "outputs": [],
      "source": [
        "def generate(prompt, model, tokenizer):\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=150, do_sample=True)\n",
        "\n",
        "    result = outputs[0][inputs[\"input_ids\"].shape[-1]:]\n",
        "    return tokenizer.decode(result, skip_special_tokens=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7ySKAcCer_o"
      },
      "source": [
        "## 2. Custom Tool üîß  \n",
        "\n",
        "For our first experiment, we'll create a custom tool that performs a simple task: adding two integer numbers.  \n",
        "\n",
        "We'll explore two possible ways of generating this behavior:\n",
        "\n",
        "1. Using a traditional string implementation\n",
        "2. Using a Chat Template\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ktu6eZx-yDm"
      },
      "source": [
        "### 2.1 Using a Traditional String Implementation\n",
        "\n",
        "The addition tool can be represented as the following Python function:  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "RBDvt95uetGL"
      },
      "outputs": [],
      "source": [
        "def add_two_integers(x: int, y: int):\n",
        "    \"\"\"\n",
        "    Adds two integer numerals\n",
        "\n",
        "    Args:\n",
        "        x: An integer\n",
        "        y: An integer\n",
        "    \"\"\"\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1b0rp9erys8"
      },
      "source": [
        "Below, we define the `system_prompt`.  \n",
        "\n",
        "The system prompt for tool calling is made up of two key components:  \n",
        "1. **`prior_system_instruction`**: This is the default foundational prompt. It describes the tool-calling functionality and outlines its workflow. If you examine it closely, you'll notice how it provides a clear explanation of how tools are integrated and utilized.  \n",
        "2. **Custom tools in JSON format**: This is a list of tools the model can access and call (e.g., the `add_two_integers` function in our example).  \n",
        "\n",
        "These two parts are combined to form the complete `system_prompt`.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "AkrvSCt5boPV"
      },
      "outputs": [],
      "source": [
        "prior_system_instruction = \"\"\"\\\n",
        "<|start_header_id|>system<|end_header_id|>\n",
        "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
        "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
        "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
        "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
        "You SHOULD NOT include any other text in the response.\n",
        "Here is a list of functions in JSON format that you can invoke.\"\"\"\n",
        "\n",
        "custom_tool = \"\"\"\\\n",
        "[\n",
        "    {\n",
        "        \"name\": \"add_two_integers\",\n",
        "        \"description\": \"Adds two integer numerals\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\"x\", \"y\"],\n",
        "            \"properties\": {\n",
        "                \"x\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"An integer\"\n",
        "                },\n",
        "                \"y\": {\n",
        "                    \"type\": \"integer\",\n",
        "                    \"description\": \"An integer\"\n",
        "                },\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\\n\"\"\"\n",
        "\n",
        "\n",
        "system_prompt = f\"{prior_system_instruction}{custom_tool}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r15uZKb5shqz"
      },
      "source": [
        "Next, we'll define the `user_prompt`, which simulates the user's input during their turn in the interaction. This prompt represents what the user would say or request, guiding the model to generate appropriate responses or perform specific actions using the defined tools.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "qUE5AAahfJOc"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"What is the result of 12322 added to 1242453\"\n",
        "\n",
        "prompt = f\"{system_prompt}{user_prompt}\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnJSJco2nlN3"
      },
      "source": [
        "Now, let's take a look at how the final prompt is structured, and generate the model's response based on it.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yndeUiJVbitb",
        "outputId": "b22bec11-462e-424d-9ef8-a3a69d829706"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start_header_id|>system<|end_header_id|>\n",
            "You are an expert in composing functions. You are given a question and a set of possible functions. \n",
            "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
            "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
            "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
            "You SHOULD NOT include any other text in the response.\n",
            "Here is a list of functions in JSON format that you can invoke.[\n",
            "    {\n",
            "        \"name\": \"add_two_integers\",\n",
            "        \"description\": \"Adds two integer numerals\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"dict\",\n",
            "            \"required\": [\"x\", \"y\"],\n",
            "            \"properties\": {\n",
            "                \"x\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"An integer\"\n",
            "                },\n",
            "                \"y\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"An integer\"\n",
            "                },\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the result of 12322 added to 1242453\n",
            "\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwdSKBtn8yHl"
      },
      "source": [
        "As you can see, the different special tokens (`<|eot_id|>`, `<|start_header_id|>`, etc.) indicate the turns in the conversation and help manage the workflow.\n",
        "\n",
        "Now, let's use this prompt as input to the model and check if it correctly understands that the tool needs to be called.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJDbD2aabR9-",
        "outputId": "0f35327b-245b-4928-d508-397d99943cbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[add_two_integers(x=12322, y=1242453)]<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "model_tool_call_response = generate(prompt, model, tokenizer)\n",
        "print(model_tool_call_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tcFB5jsnwjZ"
      },
      "source": [
        "As we can see, the model correctly understood the task and suggested a call to the custom tool, following the format we previously defined.  \n",
        "\n",
        "However, it's important to note that this is not the final response from the model. Tool calling occurs automatically, without any direct intervention from the user.  \n",
        "\n",
        "In a chat service, for example, we'd display a progress indicator and possibly a message indicating that additional information is being retrieved (e.g., browsing the web). The user would not see the raw response from `add_two_integers` directly.  \n",
        "\n",
        "What follows is the code required to complete the query and provide the final response. If you're interested in the underlying architecture, there's a helpful diagram illustrating it [here](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/#-tool-calling-(8b/70b/405b)-).  \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dygOg6lihHR"
      },
      "source": [
        "#### Call to the Tool  \n",
        "\n",
        "To complete the query, we'll invoke the tool to perform the operation and add the result back to the prompt. This step involves calling our previously defined function to execute the addition.  \n",
        "\n",
        "It's important to note that this process is managed outside of the model, handled by a **code executor**. The model generates the method signature, but we need to execute the actual call externally to complete the operation.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_2W6Oqtiigt",
        "outputId": "f799fb63-bf40-4dad-a8f1-bed880064759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1254775\n"
          ]
        }
      ],
      "source": [
        "tool_call_response = str(add_two_integers(x=12322, y=1242453))\n",
        "print(tool_call_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIZJwcLMim0A"
      },
      "source": [
        "#### Add Everything to the Main Prompt  \n",
        "\n",
        "Now that we have the result, we can combine all the components into the complete prompt for the final output. This includes:  \n",
        "- The **initial system prompt**  \n",
        "- The **user input**  \n",
        "- The **tool's response**  \n",
        "\n",
        "These elements together form the full interaction for the model.  \n",
        "\n",
        "It's worth noting that the `<|python_tag|>` token is simply a convention chosen to indicate that a tool call invocation is required. It doesn't strictly mean the tool must be executed in Python. We're free to use any mechanism to invoke the tool, as long as it processes the function and returns the response in the format expected by the model.  \n",
        "\n",
        "Additionally, we include the `<|start_header_id|>ipython<|end_header_id|>` token just before the tool call response to signal its context appropriately.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYk5JNrSh41q",
        "outputId": "12067008-1c68-4c79-dc66-d0266bafb9d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start_header_id|>system<|end_header_id|>\n",
            "You are an expert in composing functions. You are given a question and a set of possible functions. \n",
            "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
            "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
            "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
            "You SHOULD NOT include any other text in the response.\n",
            "Here is a list of functions in JSON format that you can invoke.[\n",
            "    {\n",
            "        \"name\": \"add_two_integers\",\n",
            "        \"description\": \"Adds two integer numerals\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"dict\",\n",
            "            \"required\": [\"x\", \"y\"],\n",
            "            \"properties\": {\n",
            "                \"x\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"An integer\"\n",
            "                },\n",
            "                \"y\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"An integer\"\n",
            "                },\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the result of 12322 added to 1242453\n",
            "\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<|python_tag|>[add_two_integers(x=12322, y=1242453)]<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
            "\n",
            "\"\\\"1254775\\\"\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"{prompt}<|python_tag|>{model_tool_call_response}<|start_header_id|>ipython<|end_header_id|>\\n\\n\"\n",
        "\n",
        "prompt = prompt + f'\"\\\\\"{tool_call_response}\\\\\"\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqYqMUx7oaSe"
      },
      "source": [
        "Finally, we'll call the model again using the complete prompt to see how it performs! This step combines all the elements into a single input, allowing the model to generate its final output.  \n",
        "\n",
        "The results, excluding the `<|eot_id|>` token, would represent the actual response provided to the user in a real application scenario. This ensures a seamless experience where the tool's execution is invisible to the end user, and only the refined answer is displayed.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUlt_VmUirMh",
        "outputId": "2699f2fd-eb19-49bb-8ac0-1583d2b77e5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The result of adding 12322 to 1242453 is 1254775.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(prompt, model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtDiX7s0q0aM"
      },
      "source": [
        "### 2.2 Using a Chat Template  \n",
        "\n",
        "The traditional string implementation, while functional, is more prone to errors since we must carefully manage tokens, line breaks, and formatting. To simplify this process, we can use a **chat template**. This approach integrates the interactions seamlessly, making it easier to build applications without worrying about the underlying tokenization structure.  \n",
        "\n",
        "You can find more information about chat templates [here](https://huggingface.co/docs/transformers/main/chat_templating).  \n",
        "\n",
        "In the following example, we'll demonstrate how to replicate the same behavior as before using the `chat_template` functionality. The code cell below represents the chat structure we need to interact with the model, implemented as a Jinja template. While it may seem complex initially, the final result is identical to what we achieved with the traditional approach.  \n",
        "\n",
        "üßë‚Äçüè´Ô∏è Let's dive in and see it in action!  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "wIF-XpmLq1kf"
      },
      "outputs": [],
      "source": [
        "chat_template=\"\"\"\\\n",
        "{{- bos_token }}\n",
        "{%- if custom_tools is defined %}\n",
        "    {%- set tools = custom_tools %}\n",
        "{%- endif %}\n",
        "{%- if not tools_in_user_message is defined %}\n",
        "    {%- set tools_in_user_message = false %}\n",
        "{%- endif %}\n",
        "{%- if not date_string is defined %}\n",
        "    {%- if strftime_now is defined %}\n",
        "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
        "    {%- else %}\n",
        "        {%- set date_string = \"24 September 2024\" %}\n",
        "    {%- endif %}\n",
        "{%- endif %}\n",
        "{%- if not tools is defined %}\n",
        "    {%- set tools = none %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
        "{%- if messages[0]['role'] == 'system' %}\n",
        "    {%- set system_message = messages[0]['content']|trim %}\n",
        "    {%- set messages = messages[1:] %}\n",
        "{%- else %}\n",
        "    {%- set system_message = \"\" %}\n",
        "{%- endif %}\n",
        "\n",
        "{#- System message + environment setup (modified for 3.2) #}\n",
        "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
        "{%- if tools is not none %}\n",
        "    {{- \"You are an expert in composing functions. You are given a question and a set of possible functions.\\n\" }}\n",
        "    {{- \"Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\\n\" }}\n",
        "    {{- \"If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,\\n\" }}\n",
        "    {{- \"also point it out. You should only return the function call in tools call sections.\\n\\n\" }}\n",
        "    {{- \"If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\" }}\n",
        "    {{- \"You SHOULD NOT include any other text in the response.\\n\\n\" }}\n",
        "    {{- \"Here is a list of functions in JSON format that you can invoke.\\n\\n\" }}\n",
        "    {{- \"[\\n\" }}\n",
        "    {%- for t in tools %}\n",
        "        {%- set tool_json = t['function'] | tojson(indent=4) %}\n",
        "        {{- \"    \" + tool_json | replace('\\n', '\\n    ') }}\n",
        "        {{- \",\\n\" if not loop.last else \"\\n\" }}\n",
        "    {%- endfor %}\n",
        "    {{- \"]\\n\" }}\n",
        "{%- elif builtin_tools is defined %}\n",
        "    {{- \"Environment: ipython\\n\" }}\n",
        "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") }}\n",
        "    {{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
        "    {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
        "{%- endif %}\n",
        "{{- system_message }}\n",
        "{{- \"<|eot_id|>\" }}\n",
        "\n",
        "{#- Custom tools passed in a user message (modified for consistency with 3.2) #}\n",
        "{%- if tools_in_user_message and tools is not none %}\n",
        "    {#- Extract the first user message to include tools if necessary #}\n",
        "    {%- if messages | length != 0 %}\n",
        "        {%- set first_user_message = messages[0]['content']|trim %}\n",
        "        {%- set messages = messages[1:] %}\n",
        "    {%- else %}\n",
        "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
        "    {%- endif %}\n",
        "    {{- \"<|start_header_id|>user<|end_header_id|>\\n\\n\" }}\n",
        "    {{- \"You are an expert in composing functions. You are given a question and a set of possible functions.\\n\" }}\n",
        "    {{- \"Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\\n\" }}\n",
        "    {{- \"If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,\" }}\n",
        "    {{- \"also point it out. You should only return the function call in the tools call sections.\\n\\n\" }}\n",
        "    {{- \"If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\\n\" }}\n",
        "    {{- \"You SHOULD NOT include any other text in the response.\\n\\n\" }}\n",
        "    {{- \"Here is a list of functions in JSON format that you can invoke.\" }}\n",
        "    {{- \"[\\n\" }}\n",
        "    {%- for t in tools %}\n",
        "        {%- set tool_json = t['function'] | tojson(indent=4) %}\n",
        "        {{- \"    \" + tool_json | replace('\\n', '\\n    ') }}\n",
        "        {{- \",\\n\" if not loop.last else \"\\n\" }}\n",
        "    {%- endfor %}\n",
        "    {{- \"]\\n\" }}\n",
        "    {{- first_user_message + \"<|eot_id|>\"}}\n",
        "{%- endif %}\n",
        "\n",
        "{#- Loop through messages to handle user, assistant, and tool interactions -#}\n",
        "{%- for message in messages %}\n",
        "    {%- if message.role == 'user' %}\n",
        "        {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n",
        "    {%- elif message.role == 'assistant' and 'tool_calls' in message %}\n",
        "        {#- Handle zero-shot tool calls (modified for 3.2) #}\n",
        "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "        {%- for tool_call in message.tool_calls %}\n",
        "            {{- '<|python_tag|>[' + tool_call.function.name + '(' }}\n",
        "            {%- for arg_name, arg_val in tool_call.function.arguments.items() %}\n",
        "                {{- arg_name + '=' + arg_val}}\n",
        "                {%- if not loop.last %}\n",
        "                    {{- \", \" }}\n",
        "                {%- endif %}\n",
        "            {%- endfor %}\n",
        "            {{- ')]' }}\n",
        "        {%- endfor %}\n",
        "        {{- '<|eot_id|>' }}\n",
        "    {%- elif message.role == 'assistant' %}\n",
        "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' + message['content'] | trim + '<|eot_id|>' }}\n",
        "    {%- elif message.role == 'tool' %}\n",
        "        {#- Modified for 3.2 #}\n",
        "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
        "        {%- if message.content is mapping or message.content is iterable %}\n",
        "            {{- message.content | tojson }}\n",
        "        {%- else %}\n",
        "            {{- message.content }}\n",
        "        {%- endif %}\n",
        "        {{- \"<|eot_id|>\" }}\n",
        "    {%- elif message.role == 'code' %}\n",
        "        {#- Code interpreter handling (maintained from 3.1) #}\n",
        "        {{- \"<|python_tag|>\" + message['content'] | trim + \"<|eom_id|>\" }}\n",
        "    {%- endif %}\n",
        "{%- endfor %}\n",
        "\n",
        "{#- Add the prompt for generation if specified -#}\n",
        "{%- if add_generation_prompt %}\n",
        "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
        "{%- endif %}\\\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3GJ1hLtvTPI"
      },
      "source": [
        "We can now pass this **chat template** to the tokenizer, which will simplify the process of generating responses and make the coding experience much smoother. By leveraging the template structure, the tokenizer will handle the intricacies of tokenization and formatting for us, allowing us to focus on building the functionality of our application rather than dealing with low-level details.\n",
        "\n",
        "This streamlined approach is especially useful when integrating complex interactions with language models. Let's see how we can implement it next! üöÄ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "7Q4wuBlWrC-t"
      },
      "outputs": [],
      "source": [
        "tokenizer.chat_template = chat_template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnV15_pAvchv"
      },
      "source": [
        "Now, let's generate the **prompt for our conversation**. As we can see, using **chat templates** makes the coding process more efficient and streamlined. The underlying prompt remains unchanged, but the template simplifies the integration and execution, making it easier to work with.\n",
        "\n",
        "By leveraging the template, we avoid manually managing the structure and tokens, which reduces the chances of errors and makes the whole process more flexible. Let's take a look at how it works in practice! ‚ú®\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jp2K3OO9rJDM",
        "outputId": "c981275c-290b-4192-85e0-274de3d45b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
            "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
            "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,\n",
            "also point it out. You should only return the function call in tools call sections.\n",
            "\n",
            "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
            "You SHOULD NOT include any other text in the response.\n",
            "\n",
            "Here is a list of functions in JSON format that you can invoke.\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"name\": \"add_two_integers\",\n",
            "        \"description\": \"Adds two integer numerals\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"object\",\n",
            "            \"properties\": {\n",
            "                \"x\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"An integer\"\n",
            "                },\n",
            "                \"y\": {\n",
            "                    \"type\": \"integer\",\n",
            "                    \"description\": \"An integer\"\n",
            "                }\n",
            "            },\n",
            "            \"required\": [\n",
            "                \"x\",\n",
            "                \"y\"\n",
            "            ]\n",
            "        }\n",
            "    }\n",
            "]\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "What is the result of 12322 added to 1242453<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<|python_tag|>[add_two_integers(x=12322, y=1242453)]<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
            "\n",
            "\"\\\"1254775\\\"\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the result of 12322 added to 1242453\"},\n",
        "    {\"role\": \"assistant\", \"tool_calls\": [{\"type\": \"function\", \"function\": {\"name\": \"add_two_integers\", \"arguments\": {\"x\": \"12322\", \"y\": \"1242453\"}}}]},\n",
        "    {\"role\": \"tool\", \"name\": \"add_two_integers\", \"content\": \"\\\"1254775\\\"\"},\n",
        "]\n",
        "\n",
        "prompt = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tools=[add_two_integers],\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85kKMeHXvu2Z"
      },
      "source": [
        "Now, let's generate the final answer! This will be the model's complete response, incorporating all the steps and tools we've defined throughout the process.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wfgmCpSrKZ-",
        "outputId": "14700f7d-4ea1-49c9-bac6-470cbd8e9a41"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The result is 1254775.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(prompt, model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvWBK8iT4uAs"
      },
      "source": [
        "## 3. Web Search Custom Tool üåê  \n",
        "\n",
        "In the previous example, we explored basic tool calling capabilities. Now, let's take it a step further and add a **web search** custom tool to the model.  \n",
        "\n",
        "As the [documentation](https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2#-tool-calling-(1b/3b)-) mentions:  \n",
        "\n",
        "> **Note**: Unlike the larger Llama 3.1 models (8B/70B/405B), the lightweight models do not support built-in tools like Brave Search and Wolfram.  \n",
        "\n",
        "The previous version of the Llama model included some support for built-in tools, such as Brave Search, for searching the web with a given query. Although the model doesn't come with these capabilities by default, let's see if we can integrate a web search tool and extend its functionality.  \n",
        "\n",
        "We will use the `BraveSearch` functionality directly implemented in `llama_stack`. It includes everything we need to make the query, so we will use it for simplicity and seamless integration.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "WaRcpLYx4uYd"
      },
      "outputs": [],
      "source": [
        "from llama_stack.providers.inline.agents.meta_reference.tools.builtin import BraveSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqCilfaD7SiE"
      },
      "source": [
        "The **Brave search tool** requires an API key to function. You can sign up for a free subscription (credit card required) [here](https://api.search.brave.com/register). It has a rate limit of 1 query per second, which is sufficient for development.\n",
        "\n",
        "**Important:** Never write your API key directly in the source code, as this is highly insecure. Instead, we will use **Google Colab secrets** to store the key safely. If you're running the notebook locally, you can store the key in an environment variable for secure access.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "ydxxTiv459rG"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "brave = BraveSearch(userdata.get('BRAVE_API_KEY'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDpPW4-Sxdxa"
      },
      "source": [
        "The API to run the tool can be found [here](https://github.com/meta-llama/llama-stack/blob/main/llama_stack/providers/inline/agents/meta_reference/tools/builtin.py).  \n",
        "\n",
        "To perform the web search, we'll invoke the `brave.search` function, which is part of the Brave search tool integration.  \n",
        "\n",
        "Let's see an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "5TQvvWXQxS3Q",
        "outputId": "5cc6a3cf-082e-4b6b-9693-4b6f1ebdbe83"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'{\"query\": \"La Liga 24-25 top scorer\", \"top_k\": [{\"title\": \"LaLiga - List of goalscorers 24/25 | Transfermarkt\", \"url\": \"https://www.transfermarkt.com/jumplist/torschuetzenliste/wettbewerb/ES1\", \"description\": \"This statistic show the goal <strong>scorer</strong> list of the LaLiga in the <strong>24</strong>/<strong>25</strong>, sorted by the amount of goals.\", \"type\": \"search_result\"}, {\"title\": \"La Liga top scorers 2024-25: Robert Lewandowski, Iago Aspas & the race for the Pichichi trophy | Goal.com\", \"url\": \"https://www.goal.com/en/lists/la-liga-top-scorers-2024-25/bltbd524ffd030cff7b\", \"description\": \"<strong>Artem Dovbyk</strong> scored 24 goals for Girona last season and became the top goal scorer before leaving for Serie A in the summer transfer window. There was tough competition from Alexander Sorloth who had 23 goals to his name. Both Lewandowski and Jude Bellingham had 19 goals to their name and will ...\", \"type\": \"search_result\"}, {\"title\": \"La Liga top goal scorers 2024/25: Updated Golden Boot rankings in Spain as Lewandowski leads Mbappe in Pichichi chase | Sporting News\", \"url\": \"https://www.sportingnews.com/us/soccer/news/la-liga-top-goal-scorer-updated-golden-boot-ranking-pichichi/4b054951c120586936147227\", \"description\": \"<strong>Artem Dovbyk</strong> of Girona won the Golden Boot for the 2023/24 La Liga season. This time around, Kylian Mbappe and Robert Lewandowski are poised for a showdown across the Clasico divide.\", \"type\": \"search_result\"}]}'"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tool_call_response = await brave.search(\"La Liga 24-25 top scorer\")\n",
        "tool_call_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioHA5evXydr2"
      },
      "source": [
        "As we can see, it returns the query response after searching the web. We may use this response in our experiment.\n",
        "\n",
        "Let's define the custom web search tool. It must follow the same **JSON structure** as the previous tool. Once defined, we can integrate it into the `system_prompt` to enable the model to use it during the conversation.\n",
        "\n",
        "Here is the structure we'll follow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K76dxgB16A2r",
        "outputId": "86ecd781-fef3-4db1-9c90-ed81d30d44e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start_header_id|>system<|end_header_id|>\n",
            "You are an expert in composing functions. You are given a question and a set of possible functions. \n",
            "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
            "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
            "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
            "You SHOULD NOT include any other text in the response.\n",
            "Here is a list of functions in JSON format that you can invoke.[\n",
            "    {\n",
            "        \"name\": \"web_search\",\n",
            "        \"description\": \"Search the web for a given query.\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"dict\",\n",
            "            \"required\": [\"query\"],\n",
            "            \"properties\": {\n",
            "                \"query\": {\n",
            "                    \"type\": \"str\",\n",
            "                    \"description\": \"The query to search for\"\n",
            "                },\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "custom_tool = \"\"\"\\\n",
        "[\n",
        "    {\n",
        "        \"name\": \"web_search\",\n",
        "        \"description\": \"Search the web for a given query.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\"query\"],\n",
        "            \"properties\": {\n",
        "                \"query\": {\n",
        "                    \"type\": \"str\",\n",
        "                    \"description\": \"The query to search for\"\n",
        "                },\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\\n\"\"\"\n",
        "\n",
        "system_prompt = f\"{prior_system_instruction}{custom_tool}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
        "\n",
        "print(system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_HRwvMYyxQ9"
      },
      "source": [
        "Now, let's add the `user_prompt` in the user turn, just as we did before. This will allow the model to understand the user's request and trigger the web search tool accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "U9j2vRKh7ih8"
      },
      "outputs": [],
      "source": [
        "user_prompt = \"Who is the current top-scorer of La Liga 24-25?\"\n",
        "\n",
        "prompt = f\"{system_prompt}{user_prompt}\\n\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMcmLRUn7knU",
        "outputId": "7898da98-dd56-4a16-b862-2ce55961985a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start_header_id|>system<|end_header_id|>\n",
            "You are an expert in composing functions. You are given a question and a set of possible functions. \n",
            "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
            "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
            "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
            "You SHOULD NOT include any other text in the response.\n",
            "Here is a list of functions in JSON format that you can invoke.[\n",
            "    {\n",
            "        \"name\": \"web_search\",\n",
            "        \"description\": \"Search the web for a given query.\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"dict\",\n",
            "            \"required\": [\"query\"],\n",
            "            \"properties\": {\n",
            "                \"query\": {\n",
            "                    \"type\": \"str\",\n",
            "                    \"description\": \"The query to search for\"\n",
            "                },\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Who is the current top-scorer of La Liga 24-25?\n",
            "\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv0oSpkzy6vN"
      },
      "source": [
        "Now that we have everything set up, let's call the model and see if it correctly understands the tool structure and invokes the **web search tool** as intended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dkiOXT-7m5n",
        "outputId": "79ac741e-9f3d-4020-ab7a-4e9780fff79a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[web_search(query=\"La Liga 24-25 top scorer\")]<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "model_tool_call_response = generate(prompt, model, tokenizer)\n",
        "print(model_tool_call_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mext_jI-zD-J"
      },
      "source": [
        "The model correctly understood that it needs to call the **web search tool**. Now that we have the tool's answer, we can add it to the prompt to complete the interaction and generate the final response. Since we alredy have the `brave_search_response`, we can easily add it!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CrORexW7rF3",
        "outputId": "c1b49925-ddd6-4e0d-a9bb-3ab81f9068aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|start_header_id|>system<|end_header_id|>\n",
            "You are an expert in composing functions. You are given a question and a set of possible functions. \n",
            "Based on the question, you will need to make one or more function/tool calls to achieve the purpose. \n",
            "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
            "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
            "You SHOULD NOT include any other text in the response.\n",
            "Here is a list of functions in JSON format that you can invoke.[\n",
            "    {\n",
            "        \"name\": \"web_search\",\n",
            "        \"description\": \"Search the web for a given query.\",\n",
            "        \"parameters\": {\n",
            "            \"type\": \"dict\",\n",
            "            \"required\": [\"query\"],\n",
            "            \"properties\": {\n",
            "                \"query\": {\n",
            "                    \"type\": \"str\",\n",
            "                    \"description\": \"The query to search for\"\n",
            "                },\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]\n",
            "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Who is the current top-scorer of La Liga 24-25?\n",
            "\n",
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<|python_tag|>[web_search(query=\"La Liga 24-25 top scorer\")]<|eot_id|><|start_header_id|>ipython<|end_header_id|>\n",
            "\n",
            "{\"query\": \"La Liga 24-25 top scorer\", \"top_k\": [{\"title\": \"LaLiga - List of goalscorers 24/25 | Transfermarkt\", \"url\": \"https://www.transfermarkt.com/jumplist/torschuetzenliste/wettbewerb/ES1\", \"description\": \"This statistic show the goal <strong>scorer</strong> list of the LaLiga in the <strong>24</strong>/<strong>25</strong>, sorted by the amount of goals.\", \"type\": \"search_result\"}, {\"title\": \"La Liga top scorers 2024-25: Robert Lewandowski, Iago Aspas & the race for the Pichichi trophy | Goal.com\", \"url\": \"https://www.goal.com/en/lists/la-liga-top-scorers-2024-25/bltbd524ffd030cff7b\", \"description\": \"<strong>Artem Dovbyk</strong> scored 24 goals for Girona last season and became the top goal scorer before leaving for Serie A in the summer transfer window. There was tough competition from Alexander Sorloth who had 23 goals to his name. Both Lewandowski and Jude Bellingham had 19 goals to their name and will ...\", \"type\": \"search_result\"}, {\"title\": \"La Liga top goal scorers 2024/25: Updated Golden Boot rankings in Spain as Lewandowski leads Mbappe in Pichichi chase | Sporting News\", \"url\": \"https://www.sportingnews.com/us/soccer/news/la-liga-top-goal-scorer-updated-golden-boot-ranking-pichichi/4b054951c120586936147227\", \"description\": \"<strong>Artem Dovbyk</strong> of Girona won the Golden Boot for the 2023/24 La Liga season. This time around, Kylian Mbappe and Robert Lewandowski are poised for a showdown across the Clasico divide.\", \"type\": \"search_result\"}]}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"{prompt}<|python_tag|>{model_tool_call_response}<|start_header_id|>ipython<|end_header_id|>\\n\\n\"\n",
        "\n",
        "prompt = prompt + f'{tool_call_response}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ0_WT5fzj54"
      },
      "source": [
        "Finally, we can generate the model's response, incorporating the web search tool's answer into the prompt and completing the process.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6sZQCmS7y0N",
        "outputId": "30699a1d-d1ea-4866-dd87-dcfe097cd53d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current top scorer of La Liga 24-25 is Robert Lewandowski.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "print(generate(prompt, model, tokenizer))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fgzm9AYGr6C"
      },
      "source": [
        "We are now able to add a web search tool to the model capabilities! üåê"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14524c078b48471896597f97a071fc35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1497c470009b486dba1dca51a9d2ce60": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "151fd97a9c7149eb8c97c5ba48b7a6d6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "228ffff7dcaf4ac593364194c53d3547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_1497c470009b486dba1dca51a9d2ce60",
            "style": "IPY_MODEL_3cd7c5bc91854604b3c96af7e4c1f464",
            "value": true
          }
        },
        "33cef11df22f444e98043a408943c91a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "3cd7c5bc91854604b3c96af7e4c1f464": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "601ca2c9b03946cc83600c518ab69b55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60daa7077c2344eeb0538860d0d64e26": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "695d84785a7f43feabcb5571cec4c260": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76d34dad28ce45e48806a4b829898819": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_33cef11df22f444e98043a408943c91a"
          }
        },
        "784fecb071f84b628bb6f38c3311b880": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_151fd97a9c7149eb8c97c5ba48b7a6d6",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7e201d782ac24d578a9476e37c01240c",
            "value": "Connecting..."
          }
        },
        "7e201d782ac24d578a9476e37c01240c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a57d8cef1dfe438f8a244bd0e546a04f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad32ecb235c5422e846147b8c6481903": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_601ca2c9b03946cc83600c518ab69b55",
            "style": "IPY_MODEL_60daa7077c2344eeb0538860d0d64e26",
            "tooltip": ""
          }
        },
        "cb4e63441d2e45be9d5e6804e82830b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a57d8cef1dfe438f8a244bd0e546a04f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e6ef504d91884912bb6f19ded832783c",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "cc4303145b234066a517330b66fed071": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6baa1b9954a4cd9bbd04179ab675a9b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d98183520d9441ed860171ac55ad786f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "d98183520d9441ed860171ac55ad786f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e107d71395394b6cac001a5e323ef6b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_695d84785a7f43feabcb5571cec4c260",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_14524c078b48471896597f97a071fc35",
            "value": ""
          }
        },
        "e6baa1b9954a4cd9bbd04179ab675a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6ef504d91884912bb6f19ded832783c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
