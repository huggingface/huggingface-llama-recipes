{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyNYYY/vwph6rjyuUkmXkFdk"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Speedup Llama 3 70B with Speculative Decoding\n",
        "\n",
        "In this guide, we'll show you how to implement speculative decoding with Llama 3.1 70B model (base) and Llama 3.2 3B model (assistant). Transformers has a `generate` API where we pass an `assistant_model` to enable speculative decoding. By the end, you'll see how this technique can significantly speed up text generation (upto 2x), making your workflows faster and more efficient."
      ],
      "metadata": {
        "id": "hoB6HZTw4fvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Setup"
      ],
      "metadata": {
        "id": "8h73s8MaxqrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq transformers accelerate"
      ],
      "metadata": {
        "id": "NaWzzUs5yfwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import time\n",
        "import torch\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "6Kp5KI7qxsXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# supress the warning in the notebook\n",
        "import logging\n",
        "import warnings\n",
        "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "q5lpixXsrJQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# base models\n",
        "checkpoint = \"meta-llama/Meta-Llama-3.1-70B\"      # <-- Larger Model\n",
        "assistant_checkpoint = \"meta-llama/Llama-3.2-3B\"  # <-- Smaller Model"
      ],
      "metadata": {
        "id": "IUEYY5j71ph0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Models"
      ],
      "metadata": {
        "id": "FyUZ7dpBmVjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "assistant_model = AutoModelForCausalLM.from_pretrained(\n",
        "    assistant_checkpoint,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")"
      ],
      "metadata": {
        "id": "LihenVlfgsuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Inputs"
      ],
      "metadata": {
        "id": "yE5QDkOOmZ64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Alice and Bob\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)"
      ],
      "metadata": {
        "id": "na-Qefklg0nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Benchmark the text generation speed\n"
      ],
      "metadata": {
        "id": "QTXUxJlWmieJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_new_tokens = 256\n",
        "num_iterations = 10"
      ],
      "metadata": {
        "id": "nZQr3rSJg-eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoding"
      ],
      "metadata": {
        "id": "9t9jh24rrdYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# warmup\n",
        "for _ in range(2):\n",
        "    model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample=False,\n",
        "        assistant_model=assistant_model,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=-1,\n",
        "    )"
      ],
      "metadata": {
        "id": "mP8co683-T2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without assistance\n",
        "print(\"ðŸ˜ Big Model Generation\")\n",
        "duration = 0\n",
        "for _ in tqdm(range(num_iterations)):\n",
        "    start = time.time()\n",
        "    outputs = model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample=False,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=-1,\n",
        "    )\n",
        "    end = time.time()\n",
        "    duration = duration + (end-start)\n",
        "\n",
        "print(f\"\\nThroughput: {(num_iterations * max_new_tokens) / (duration):.4f} (tokens/sec)\")\n",
        "\n",
        "\n",
        "# with assistance\n",
        "print(\"\\n\\nðŸ¤ Big Model Generation with Assistance\")\n",
        "duration = 0\n",
        "for _ in tqdm(range(10)):\n",
        "    start = time.time()\n",
        "    outputs = model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample=False,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=-1,\n",
        "        assistant_model=assistant_model,\n",
        "    )\n",
        "    end = time.time()\n",
        "    duration = duration + (end-start)\n",
        "\n",
        "print(f\"\\nThroughput: {(num_iterations * max_new_tokens) / (duration):.4f} (tokens/sec)\")"
      ],
      "metadata": {
        "id": "O482aPT-47xH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multinomial Decoding"
      ],
      "metadata": {
        "id": "fu8FmSpUrgU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# warmup\n",
        "for _ in range(2):\n",
        "    model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        assistant_model=assistant_model,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=-1,\n",
        "    )"
      ],
      "metadata": {
        "id": "wd6gbarl8GTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# without assistance\n",
        "print(\"ðŸ˜ Big Model Generation (multinomial)\")\n",
        "duration = 0\n",
        "for _ in tqdm(range(num_iterations)):\n",
        "    start = time.time()\n",
        "    outputs = model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=-1,\n",
        "    )\n",
        "    end = time.time()\n",
        "    duration = duration + (end-start)\n",
        "\n",
        "print(f\"\\nThroughput: {(num_iterations * max_new_tokens) / (duration):.4f} (tokens/sec)\")\n",
        "\n",
        "\n",
        "# with assistance\n",
        "print(\"\\n\\nðŸ¤ Big Model Generation with Assistance (multinomial)\")\n",
        "duration = 0\n",
        "for _ in tqdm(range(10)):\n",
        "    start = time.time()\n",
        "    outputs = model.generate(\n",
        "        **model_inputs,\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        max_new_tokens=256,\n",
        "        eos_token_id=-1,\n",
        "        assistant_model=assistant_model,\n",
        "    )\n",
        "    end = time.time()\n",
        "    duration = duration + (end-start)\n",
        "\n",
        "print(f\"\\nThroughput: {(num_iterations * max_new_tokens) / (duration):.4f} (tokens/sec)\")"
      ],
      "metadata": {
        "id": "asU-_sjOgeJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "\n",
        "| | Base Model Throughput | |\n",
        "| :-- | :-- | --:\n",
        "| | simple | assisted |\n",
        "| greedy | 4.9464 | **9.7564** |\n",
        "| multinomial | 4.9309 | **6.2531** |\n",
        "\n",
        "\n",
        "The throughput increases with assisted generation! ðŸŽ‰\n",
        "\n",
        "While this process gains speed, it often comes at the cost of increased memory usage, so it's important to balance both metrics."
      ],
      "metadata": {
        "id": "N3qi22BqzzQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps\n",
        "\n",
        "To know more about speculative decoding we suggest reading:\n",
        "\n",
        "- [Assisted Generation](https://huggingface.co/blog/assisted-generation): Learn more about assisted generation.\n",
        "- [Speculative Decoding Docs](https://huggingface.co/docs/transformers/main/en/generation_strategies#speculative-decoding): See how transformers does decoding with an assistant.\n",
        "\n",
        "## Acknowledgements\n",
        "\n",
        "1. [Vaibhav Srivastav](https://huggingface.co/reach-vb) for the thorough review and suggestions to make the tutorial better.\n",
        "2. [Joao Gante](https://huggingface.co/joaogante) for clarifying my doubts on speculative decoding."
      ],
      "metadata": {
        "id": "jeYf-CTXrv0k"
      }
    }
  ]
}